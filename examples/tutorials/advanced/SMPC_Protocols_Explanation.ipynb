{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMPC Protocols Explanation \n",
    "This notebook will give you an overview and a very quick explanation of/over the different SMPC protocols that are currently implemented in PySyft. It'll also elaborate on **what kind of Machine Learning you can conduct in an encrypted fashion using them**, along with a comparison of their resulting performance to each other and to the non-encrypted scenario. The protocol explanations should mainly serve to give you a high-level understanding of what the main **crypto-slang** stands for, how the terms relate to each other and what resources serve as good starting points to dig deeper! \n",
    "\n",
    "Authors:\n",
    "- Nicolas Remerscheid - GitHub: [@NiWaRe](https://github.com/NiWaRe)\n",
    "\n",
    "\n",
    "## Quick recap - What is SMPC encryption?\n",
    "As a quick recap, \"SMPC\" stands for **Secure Multi-Party Computation** and constitutes a form of encryption that can be used for Machine Learning (i.e. it is possible to do calculations on encrypted data) leveraging a network of min. 2 different servers. These systems are typically resistant to some level of **collusion.** This means that usually it is considered that there exists a *honest majority* of servers which are trusted of not diverting from the given protocol - e.g. not collaborating with each other. As a whole the servers work as a **trusted execution environment** on which sensitive calculations such as model inference, training, etc. can be done without the model or the data being disclosed to any party besides the respective owner. \n",
    "\n",
    "## General concepts \n",
    "* Important concepts that all protocols are based upon - *in brief:* \n",
    "  * **A Public value,** is considered data (e.g. input from the data-sources) which is known by all parties. \n",
    "  * **A Private value,** is considered data which is secured through additive secret sharing, only the owner knows the true value. \n",
    "  * **Masking:** To share a private value publicly (e.g. as a necessary part in a protocol) it has to be masked first. This typically simply involves adding a random number to the value and projecting it onto a fixed set of numbers, a so-called ring. <br>  *masked_value = (private_value - random_numb) % upper absolute value of set of numbers.* For more info on that check out the Udacity Beginner Tutorials by Andrew Trask. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use-Case: Cifar10 Classification with CNN\n",
    "* To tackle a problem similar to a real-world-problem, yet still using a well explored example (the privacy tools are of main interest in this tutorial) we choose to *train* an Image-Classifier on the **[Cifar-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)** and *do inference* on it. <br>\n",
    "* Containing 60000 32x32 pixels coloured images of 10 different classes (airplanes, birds, etc.) this should showcase a reasonably similair task to other real-world examples such as the training of a classifier for skin-cancer-classification, which heavily relies on sensitive private data. See [Stanford's Skin Cancer Classification with Deep Learning](https://cs.stanford.edu/people/esteva/nature/) for more information on this specific example.\n",
    "* *Specific characteristics:*\n",
    "  * We consider a system of **two** servers as a secure computation unit on which all the computations are conducted in a safe manner. In this case we consider the scenario where two data owners train a model (possibly from a third party) in an encrypted fashion on their respective devices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup of example use-case problem \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler \n",
    "\n",
    "import numpy as np \n",
    "import syft as sy\n",
    "import time\n",
    "import tqdm as tqdm \n",
    "\n",
    "# Extend torch functionality with PySyft\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Get Data ##\n",
    "\n",
    "_ = torch.manual_seed(1234)\n",
    "batch_size = 32\n",
    "\n",
    "# download Cifar-10 Dataset and distribute onto two servers \n",
    "\n",
    "# normalize data and convert to torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "# get Cifar10 Dataset from torchvision.datasets\n",
    "cifar10_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "\n",
    "cifar10_data_test = datasets.CIFAR10('data', train=False,\n",
    "                              download=True, transform=transform)\n",
    "\n",
    "# create DataLoaders \n",
    "cifar10_train_loader = torch.utils.data.DataLoader(cifar10_data, batch_size=batch_size)\n",
    "cifar10_test_loader = torch.utils.data.DataLoader(cifar10_data_test, batch_size=batch_size)\n",
    "\n",
    "# image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup distributed scenario ##\n",
    "\n",
    "# create VirtualWorkers - could be any participating node \n",
    "sam = sy.VirtualWorker(hook, id=\"sam\")\n",
    "kelly = sy.VirtualWorker(hook, id=\"kelly\")\n",
    "workers = [sam, kelly]\n",
    "\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove compression to have faster communication, because compression time \n",
    "# is non-negligible: we send to workers crypto material which is very heavy\n",
    "# and pseudo-random, so compressing it takes a long time and isn't useful:\n",
    "# randomness can't be compressed, otherwise it wouldn't be random!\n",
    "from syft.serde.compression import NO_COMPRESSION\n",
    "sy.serde.compression.default_compress_scheme = NO_COMPRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Inference - Quick Demo\n",
    "We can do the encryption for data which is stored on our own device (node) or we can remotely encrypt data which is stored on other devices (for example for an Data-Centric Federated Learning with Secure Aggregation use-case with real workers, see [Part 10 - Federated Learning with Secure Aggregation](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb), see this blog post for more about [different kinds of Federated Learning](https://blog.openmined.org/federated-learning-types/)). As an example we'll take Resnet18 as a very popular and powerful model where encrypted inference (forward pass) is already supported! (in `.eval()` mode!) \n",
    "<br><br>\n",
    "Feel free to change the protocol from *Functional Secret Sharing* `fss` to *Secure NN* `snn`. More on these two protocols in the following. \n",
    "<br><br>\n",
    "*Note:* All computations were conducted on Mac OS Catalina, 2 GHz Quad-Core Intel Core i7, 16 GB RAM (MBP late 2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model - resnet18 \n",
    "# model weights for a resnet trained on Cifar10: https://github.com/huyvnphan/PyTorch_CIFAR10\n",
    "\n",
    "model = models.resnet18(pretrained=True).eval()\n",
    "\n",
    "# Normally you would now train this last classification layer\n",
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encryption parameters - feel free to change these  \n",
    "\n",
    "encryption_kwargs = dict(\n",
    "    workers=workers, \n",
    "    crypto_provider=crypto_provider, \n",
    "    protocol=\"fss\", \n",
    "    requires_grad=True,\n",
    "    precision_fractional= 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remotely encrypt one batch and the model\n",
    "first_batch, first_target = next(iter(cifar10_train_loader))\n",
    "\n",
    "# we assume sam has some data and the model\n",
    "# (of course you can also skip this and assume the data is stored locally)\n",
    "ptr_first_batch = first_batch.send(sam)\n",
    "ptr_model = model.send(sam)\n",
    "\n",
    "# .get() because a encrypt() returns a pointer from me->sam where AdditiveSharingTensor is stored\n",
    "# not necessary if you encrypt data which is on your local device \n",
    "encrypted_first_batch = ptr_first_batch.encrypt(**encryption_kwargs).get()\n",
    "encrypted_model = ptr_model.encrypt(**encryption_kwargs).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the following two lines, you'll see the internal structure of the encrypted data. The pointer on the shared data is structured into three different tensor-types which also reflect the specific actions we just did to get to an encrypted tensor. To see how they're implemented click on the respective tensor. *More on the specifics in the following!* \n",
    "* [`AutogradTensor`](https://github.com/OpenMined/PySyft/blob/d811ef1e91e5e2c84fbbf1edf61e6983380b4d16/syft/frameworks/torch/tensors/interpreters/autograd.py#L29) - first, we want to have a tensor on which all computations are tracked for later backprop. \n",
    "* [`FixedPrecisionTensor`](https://github.com/OpenMined/PySyft/blob/d811ef1e91e5e2c84fbbf1edf61e6983380b4d16/syft/frameworks/torch/tensors/interpreters/precision.py#L19) - second, we need to convert all numbers from floats to fixed-point integers for the encryption in the next step.\n",
    "* [`AdditiveSharingTensor`](https://github.com/OpenMined/PySyft/blob/d811ef1e91e5e2c84fbbf1edf61e6983380b4d16/syft/frameworks/torch/tensors/interpreters/additive_shared.py#L63) - lastly, we encrypted the data by parting it into different chunks and distributing it among the workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       "\t-> [PointerTensor | me:92566034500 -> sam:59900641324]\n",
       "\t-> [PointerTensor | me:55213798748 -> kelly:4500639760]\n",
       "\t*crypto provider: crypto_provider*"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypted_first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nicolas/miniconda3/envs/pp-ml/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py:122: UserWarning: Use dtype instead of field\n",
      "  warnings.warn(\"Use dtype instead of field\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference done in 181.70145916938782 sec. Privacy preserving fetching of prediction!\n",
      "Total Duration: 182.05092406272888\n",
      "Result: tensor([2., 8., 8., 6., 6., 2., 2., 5., 6., 6., 8., 2., 7., 6., 5., 6., 2., 6.,\n",
      "        8., 6., 6., 3., 8., 8., 2., 8., 6., 8., 8., 1., 1., 6.])\n"
     ]
    }
   ],
   "source": [
    "# encrypted inference - feel free to do the same with the second batch & model\n",
    "start_time = time.time()\n",
    "\n",
    "encrypted_result = encrypted_model(encrypted_first_batch)\n",
    "print(f\"Inference done in {time.time() - start_time} sec. Privacy preserving fetching of prediction!\")\n",
    "pred = encrypted_result.argmax(dim=1)\n",
    "\n",
    "print(f\"Total Duration: {time.time() - start_time}\")\n",
    "# decrypt() = get().float_precision() - the specific result isn't sensible as the ResNet wasn't actually trained \n",
    "print(f\"Result: {pred.decrypt()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SecureNN encryption took 992.60 sec. (~ 2 sec. only for the argmax operation) - 32 samples, 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Training - Cifar10\n",
    "Now we know how to do encrypted inference, let's take a look at encrypted training now. To be able to understand the possibilities and limits of encrypted training with different SMPC-protocols we're going to take a look at each of the protocols and then compare their performance for the training of an Image-Classifier on Cifar10. <br>\n",
    "We don't use the ResNet here, because not all computations are yet supported for encrypted backpropagation (Oct. 2020). Instead we use a simple custom CNN (at the start of each section), which should also give you the liberty of trying out different modules and computations and their performances on your own. Also, feel free to alter and tune the training process along with the loss function, optimizer, etc. to get a better feeling about the performance of different computations. <br>\n",
    "*Also, if you find that a critical module, loss fct., optimizer isn't supported yet feel free to create an Issue on the OpenMined Github.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serves as base for both protocols (for better comparison)\n",
    "\n",
    "class SimpleCNN_Base(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(SimpleCNN_Base, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        \n",
    "        self.pool1 = nn.AvgPool2d(2, 2)\n",
    "        self.pool2 = nn.AvgPool2d(2, 2)\n",
    "        self.pool3 = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        # after conv and pooling layer dimension: 4x4x32 (original: 32x32x3)\n",
    "        self.lin1 = nn.Linear(4*4*32, 128)\n",
    "        self.lin2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        out = self.pool1(F.relu(self.conv1(x)))\n",
    "        out = self.pool2(F.relu(self.conv2(out)))\n",
    "        out = self.pool3(F.relu(self.conv3(out)))\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.dropout(F.relu(self.lin1(out)))\n",
    "        out = F.relu(self.lin2(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions ##\n",
    "\n",
    "# One-Hot Encoding (Copied from @laRiffle Part 12)\n",
    "def one_hot_of(index_tensor):\n",
    "    \"\"\"\n",
    "    Transform to one hot tensor\n",
    "        \n",
    "    Example:\n",
    "        [0, 3, 9]\n",
    "        =>\n",
    "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "            \n",
    "    \"\"\"\n",
    "    onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for Cifar10\n",
    "    onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "    return onehot_tensor\n",
    "\n",
    "def CrossEntropyLoss(output, target):\n",
    "    \n",
    "    # LogSoftmax #\n",
    "\n",
    "    # Vectorized and in log-space with substraction instead of division is KEY! \n",
    "    # => 17 sec. per division and if using not the vectorized version ~0.5 sec. per value \n",
    "    # Only little rounding errors in scale of predefined precision occur when using vectorized version\n",
    "    log_probs = output - torch.log(torch.exp(output).sum(dim=1).unsqueeze(dim=1))\n",
    "\n",
    "    # CELoss #\n",
    "    batch_loss = torch.mean( \n",
    "                     torch.sum(  \n",
    "                        -target * log_probs, dim=1\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SPDZ Protocol \n",
    "* **Encryption on n parties possible (n >= 2)**\n",
    "* Basis for advanced protocols secureNN and FSS. \n",
    "* *In depth material:*\n",
    "  * [Bristol Cryptography Blog Series](https://bristolcrypto.blogspot.com/2016/10/what-is-spdz-part-1-mpc-circuit.html)\n",
    "  * [Morten Dahl's Blog](https://mortendahl.github.io/2017/09/03/the-spdz-protocol-part1/)\n",
    "  * [PySyft Code](https://github.com/OpenMined/PySyft/blob/master/syft/frameworks/torch/mpc/spdz.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General concept \n",
    "There are two types of SMPC: secret-sharing-based SMPC and circuit-garbling-based SMPC. \n",
    "The two protocols (secureNN and FSS) that are implemented in PySyft to date are both based on the **SPDZ protocol**, \n",
    "which is based on *additive secret-sharing* (the first category). For more information on what *additive secret-sharing* is, check out the tutorials by Andrew Trask as part of the Udacity Private and Secure AI Course.<br>\n",
    "SPDZ is a very widely used protocol for computing linear functions in an encrypted fashion, meaning it can be used to compute sums and multiplication of encrypted variables. The sum of encrypted variables simply consists of each server summing locally its shares of the private variables, which then leads to each server having a share of the sum of the private variables, thus together secret-sharing the sum of the private variables. The multiplication of two encrypted variables however is done using so-called **\"beaver multiplication triples\"** - three randomly generated numbers - which make the encrypted multiplication very efficient compared to other approaches (e.g. garbled circuits).\n",
    "<br>\n",
    "*Why more efficient?* See as a brief introduction into this topic the 'Extra: Beaver Multiplication Triples' section or to dig deeper see the resources mentioned just above. \n",
    "<br>\n",
    "<br>\n",
    "The high-level procedure of SPDZ-based protocols is as follow: (based on the definition from the [Bristol Cryptography Blog](https://bristolcrypto.blogspot.com/2016/10/what-is-spdz-part-1-mpc-circuit.html))\n",
    "   1. Parties secret-share their inputs at the beginning (one crypto-provider exists to generate beaver triples, etc.).\n",
    "   2. Parties compute mul. and sum. locally (using only their share of the private variables). *By design they don't have to communicate with each other while doing the sums locally. Only, in the end, they share their end-result. For multiplication there is one intermediate communication step to exchange hidden shares that are needed for the product to be computed (the exchange of the masked shares, as explained in the next section about Beaver Triples).*\n",
    "   3. In the end, parties reveal the result of their calculation by sending their share to one server which adds all shares up to unveil the final result. This could be any participating server, or the shares could also be mutually shared with everybody to give any participating server the possibility to decrypt the final result. (Not implemented in PySyft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Beaver Multiplication Triples\n",
    "Beaver Triples are simply put randomly generated numbers which are also shared among all different workers and are used to mask (as described above) the input variables to the multiplication so that they can be publicly shared. Then they can be only used to compute the product (the end-result) by simply computing a simple equation out of all masked input variables, which cancels out the random masks and reveals the product. So effectively no further communication between the workers is necessary during the computation itself. Now concerning the overall efficiency of the protocol, you might think that although the communication-complexity is low we still need to generate new Beaver Triples for each multiplication? (because of security issues they can only be used for one multiplication) <br>\n",
    "For this reason, however, the SPDZ protocol is deliberately parted into an 'offline' and an 'online' phase. The 'offline' phase consists of randomly generating all necessary \"crypto-primitives\" - e.g. the Beaver Triples - and can essentially be executed independently of specific inputs of the multiplication, i.e. before the actual multiplications. Thus, given we have generated enough Beaver Triples beforehand, during the 'online' phase we can compute as many multiplications as we have pre-generated Beaver Triples. It has to be noted that we could also generate necessary Beaver Triples during the 'online' phase but that would slow down the computation time for the user waiting for his multiplication to be conducted. The offline phase allows us to shorten this waiting time for the user by generating the triples when no requests are being made. *This makes the SPDZ protocol very efficient for linear computations!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Crypto-Store\n",
    "  * As mentioned above a key feature of the SPDZ-protocol is its splitting of the encrypted computation process into an *online* and an *offline* phase to allow for a significantly decreased execution time (given enough time for the offline phase when no encrypted computations are being conducted)\n",
    "  * As you probably know in PySyft there exist *worker* objects that have certain default attributes specified and set in `class BaseWorker(AbstractWorker)` ([code](https://github.com/OpenMined/PySyft/blob/c83e615a85bb8944245668d90582fb97c45e6e18/syft/workers/base.py#L48)). One of them is a so-called `worker.crypto_store`. The *crypto_store* object is of type `class PrimitiveStorage`, which specifies a set of given functions that help the respective workers to manage crypto-primitives they need to participate in the respective crypto-protocol. (e.g. Beaver Triples for multiplication in the SPDZ-protocol)\n",
    "  * Specifically there are two important methods of the crypto_store object. For the *crypto-provider* - the party that serves as a trusted, neutral participant of the protocol - the `crypto_provider.crypto_store.provide_primitives(...)` ([code](https://github.com/OpenMined/PySyft/blob/c83e615a85bb8944245668d90582fb97c45e6e18/syft/frameworks/torch/mpc/primitives.py#L161)) method generates and sends crypto-primitives such as Beaver Triples to participating workers. <br> For \"normal\" participating workers the `worker.crypto_store.get_keys(...)` ([code](https://github.com/OpenMined/PySyft/blob/c83e615a85bb8944245668d90582fb97c45e6e18/syft/frameworks/torch/mpc/primitives.py#L52)) method takes care of receiving and storing the crypto primitives for later usage during the protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Garbled Circuits \n",
    "Maybe you've read this term a couple of times on the OpenMined workspace or in SMPC-encryption discussions - *on a high-level what is circuit-garbling-based SMPC, or more specifically what are'Garbled Circuits?* <br>\n",
    "Garbled Circuits is a protocol which works as **2-Party-Computation** (i.e. only two servers are involved). The name comes from the method which is used to encrypt a function: a function is represented as a **circuit** consisting of different logical gates (e.g. AND, XOR, etc.). This circuit can be described with a truth-table which indicates at what inputs, what outputs follow. As part of the encryption process, the rows of the truth-table are re-ordered arbitrarily which leads to the name \"garbled\" circuits. <br> \n",
    "The details about the protocol (see [Wikipedia](https://en.wikipedia.org/wiki/Garbled_circuit) or [this](https://wiki.mpcalliance.org/garbled_circuit.html) comprehensive article from the MPC Wiki) are rather straight forward, the important thing to note is that this technique is **very flexible** as essentially all functions can be encrypted using this method, but also **very inefficient**. That's why often the Garbled Circuits protocol is mainly used on-top of more optimized protocols (such as e.g. SPDZ) to extend the variety of functions compatible with these protocols while not slowing down the computation too much for most computations. <br>\n",
    "It has to be noted that nevertheless, researchers try to find alternative, more efficient protocols that work without Garbled Circuits at the cost of supporting only a lower variety of functions. *Examples for protocols especially for computations that are popular in Machine Learning Models are secureNN and FSS which PySyft supports and over which I'll go over in the following section!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SecureNN Protocol\n",
    "* Introduced in [paper](https://eprint.iacr.org/2018/442.pdf): *SecureNN: 3-Party Secure Computation for Neural Network Training, by Sameer Wagh, Divya Gupta, and Nishanth Chandran, in Proceedings on Privacy Enhancing Technologies,  2019* \n",
    "* **Protocol is made for computation on 2 parties with 1 crypto-worker.** (There can still be multiple data-owners, but the computations is done on 2 servers - 2-party-additive-sharing)\n",
    "\n",
    "## High-Level concept \n",
    "SecureNN uses the SPDZ protocol for linear layers (beaver triples, etc.) and contains multiple efficient protocols for common non-linearities, as further described below. Compared to earlier work SecureNN implements the computation of the non-linearities without the need of Garbled Circuits (as is the case for SecureML, which was considered the state-of-the-art ML protocol before SecureNN). This also means \"interconversation protocols\", to bridge between encoding needed for SPDZ and encoding needed for Garbled Circuits, aren't necessary, decreasing computation time further. <br>\n",
    "*In general SecureNN is therefore a lot faster than SecureML and other garbled-circuit-based (for non-linearities) protocols.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In detail \n",
    "### Possible functions - High Level (Non exhaustive list)\n",
    "Resulting from the possible low-level computations (see the \"curious section\" below) the following standard models, optimizers and loss-functions can be used. *This should provide a useful summary, but doesn't claim to be an exhaustive list (feel free to add important items that can be composed out of the above mentioned possible low-level computations)\n",
    "\n",
    "* **Model Architecture:** Includes the possibility for encrypted computation of the derivative (needed for backprop)\n",
    "    * **Linear Layers** \n",
    "      * Matrix Multiplication and Convolutions (in CNNs e.g.)\n",
    "      * Average Pooling \n",
    "      * Batch-Norm/Normalization (Division of two private variables in general is possible)\n",
    "      * Dropout (*with help of Select Share, certain computation-results can be ignored. Or simply set some inputs to some neurons to zero.*)\n",
    "    * **Non-Linear Layers:** \n",
    "      * Max Pooling \n",
    "      * ReLU, Leaky ReLU, Piece-wise linear activation functions \n",
    "      * Argmax\n",
    "        * **Beware** of using `argmax()` on too many classes, see FSS section for more details. \n",
    "     \n",
    "* **Optimizers:** \n",
    "  * SGD (with Momentum)\n",
    "  * ADAM (Momentum + RMSProp) - *as devision is also possible (and elementwise-multiplication as well)*\n",
    "    * **Not** yet implemented\n",
    "\n",
    "* **Loss-functions:** *No native torch losses are compatible, they have to be **manually** implemented!* \n",
    "  * L1-Loss - *as max() is possible*\n",
    "  * MSE - *linear computation and power can be computed with SPDZ* \n",
    "  * Hinge-Loss (Linear Classification with Soft-Margin-SVM) - *as max(0, t) is possible*\n",
    "  * Cross-Entropy-Loss\n",
    "\n",
    "### Security Guarantees \n",
    "* Full Security includes **privacy and correctness**\n",
    "* The following guarantees hold for all settings where there is a majority of honest participating servers (**Not in dishonest majority setting!**)\n",
    "1. **Full Security for semi-honest corruption of a server** \n",
    "  * Privacy and Correctness of the data is secured if a server is being *honest-but-curious.* Meaning that follows the protocol but tries to infer as much information about the data it sees as possible. \n",
    "2. **Privacy against malicious server** \n",
    "  * Even a server which doesn't follow the given protocol can't learn anything about the inputs and outputs of the other (honest) servers, *given that the majority of the participants are honest!* This is a common assumption for the malicious case because in a real setting deviating from the given protocol could be prevented with additional measures, as seen below. \n",
    "3. Potential Add-on: **Security with Abort**\n",
    "  * Protection against malicious servers can be guaranteed by adding [MAC authentication](https://en.wikipedia.org/wiki/Message_authentication_code) to the protocol. This would allow aborting the protocol as soon as one of the servers doesn't follow the protocol anymore. \n",
    "\n",
    "\n",
    "### Performance Evaluation - Important Metrics \n",
    "* Division is possible but very slow! \n",
    "* Important Metrics: \n",
    "  * Round Complexity - *How many steps does the protocol involve. A step described as exchanging one message.* \n",
    "  * Communication Complexity - *How much bits are being exchanged during the protocol* \n",
    "* **DETAILS:** see \"curious section\" below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_SNN(SimpleCNN_Base):\n",
    "    def __init__(self): \n",
    "        super(SimpleCNN_SNN, self).__init__()\n",
    "        \n",
    "        # Conv-Layers, Lin-Layers, Dropout from base class \n",
    "        # as the functionality stays the same for FSS, SNN \n",
    "        # both based on SPDZ \n",
    " \n",
    "        # feel free to comment these out if you want to use AvgPool from the parent class\n",
    "        #self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        #self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        #self.pool3 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "        # Forward function doesn't change \n",
    "        \n",
    "simple_model_snn = SimpleCNN_SNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration for encryption: 14.94 sec.\n"
     ]
    }
   ],
   "source": [
    "# create encrypted data_loader and encrypted model \n",
    "batch_size = 32\n",
    "# To make it quick we want to train only for 2 batches\n",
    "nr_batches = 2\n",
    "nr_samples = batch_size * nr_batches\n",
    "\n",
    "encryption_kwargs = dict(\n",
    "    workers=workers, \n",
    "    crypto_provider=crypto_provider, \n",
    "    protocol=\"snn\", \n",
    "    requires_grad=True,\n",
    "    precision_fractional= 4\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Note: Here we one-hot-encode the targets for multi-class one-vs-all classification training \n",
    "encrypted_train_loader_snn = [\n",
    "        (data.encrypt(**encryption_kwargs), one_hot_of(target).encrypt(**encryption_kwargs))\n",
    "        for i, (data, target) in enumerate(cifar10_train_loader)\n",
    "        if i < nr_batches\n",
    "    ]\n",
    "\n",
    "# Note: Here we don't one-hot-encode targets, because we use targets only to calculate accuracy \n",
    "encrypted_test_loader_snn = [\n",
    "        (data.encrypt(**encryption_kwargs), target.encrypt(**encryption_kwargs))\n",
    "        for i, (data, target) in enumerate(cifar10_test_loader)\n",
    "        if i < nr_batches\n",
    "    ]\n",
    "\n",
    "encrypted_model_snn = simple_model_snn.encrypt(**encryption_kwargs)\n",
    "\n",
    "print(f\"Duration for encryption: {time.time() - start_time :.4} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Nr. 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nicolas/miniconda3/envs/pp-ml/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py:122: UserWarning: Use dtype instead of field\n",
      "  warnings.warn(\"Use dtype instead of field\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Nr. 1\n",
      "Average Sample Loss: 714, Epoch Time: 277.5244369506836\n",
      "Batch Nr. 0\n",
      "Batch Nr. 1\n",
      "Average Sample Loss: 714, Epoch Time: 271.10189628601074\n"
     ]
    }
   ],
   "source": [
    "## Training ##\n",
    "\n",
    "nr_epochs = 2\n",
    "\n",
    "# Don't forget to also convert optim params into integers (no rational numb.) so that weights also stay integers\n",
    "optimizer_snn = optim.SGD(\n",
    "    encrypted_model_snn.parameters(), lr=0.01, momentum=0.9).fix_precision(precision_fractional=4) \n",
    "\n",
    "epoch_loss = 0\n",
    "epoch_time = 0\n",
    "encrypted_model_snn.train()\n",
    "\n",
    "for epoch in enumerate(range(nr_epochs)): \n",
    "    # Start timer \n",
    "    epoch_time = time.time()\n",
    "    for i, (input, target) in enumerate(encrypted_train_loader_snn): \n",
    "        print(f\"Batch Nr. {i}\")\n",
    "        optimizer_snn.zero_grad()\n",
    "        encrypted_result_snn = encrypted_model_snn(input)\n",
    "        # Manual MSELoss\n",
    "        #batch_loss = ((encrypted_result-target)**2).sum()\n",
    "        # Manual CELoss\n",
    "        batch_loss_snn = CrossEntropyLoss(encrypted_result_snn, target)\n",
    "        batch_loss_snn.backward()\n",
    "        optimizer_snn.step()\n",
    "        epoch_loss += batch_loss_snn.get()\n",
    "        \n",
    "    print(f\"Average Sample Loss: {epoch_loss/nr_samples}, Epoch Time: {time.time() - epoch_time}\")\n",
    "    epoch_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Nr. 0\n",
      "Batch Nr. 1\n",
      "*************\n",
      "Test Accuracy: [0.140625],      Avg. Time Per Sample: 4.272 sec.\n"
     ]
    }
   ],
   "source": [
    "## Testing ##\n",
    "\n",
    "nr_correct = 0\n",
    "\n",
    "# Start timer \n",
    "start_time = time.time()\n",
    "encrypted_model_snn.eval()\n",
    "\n",
    "for i, (input, target) in enumerate(encrypted_test_loader_snn): \n",
    "    \n",
    "    print(f\"Batch Nr. {i}\")\n",
    "    \n",
    "    encrypted_result_snn = encrypted_model_snn(input)\n",
    "    \n",
    "    # calculate number of correct predictions (for accuracy) \n",
    "    encrypted_preds_snn = encrypted_result_snn.argmax(dim=1)\n",
    "    nr_correct += (encrypted_preds_snn==target).sum()\n",
    "\n",
    "batch_times = time.time() - start_time\n",
    "\n",
    "print(\"*************\")\n",
    "print(f\"Test Accuracy: {nr_correct.decrypt().numpy()/nr_samples},\\\n",
    "      Avg. Time Per Sample: {batch_times/nr_samples :.4} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functional Secret Sharing Protocol\n",
    "* Base [paper](https://link.springer.com/content/pdf/10.1007/978-3-662-46803-6_12.pdf) (first introduction): *Function secret sharing. E. Boyle, N. Gilboa, and Y. Ishai. In EUROCRYPT 2015, pages 337–367, 2015.* \n",
    "* **Encryption with n parties possible (mainly n == 2)**\n",
    "\n",
    "## High-Level Concept \n",
    "As the SecureNN protocol, the FSS protocol is also based on the SPDZ protocol for encrypted computation of *linear layers* and provides additional protocols for *non-linearities*. The fundamental difference in the FSS protocol is that instead of evaluating a public function (e.g. a ReLU activation function) at a private value (the secret shared data from the data-sources) it rather evaluates a private function at a public value. This is possible by first masking the private data (x - r mod Q) and then making it publicly available to obtain a public value and to then secret-share the function. <br> \n",
    "*How exactly do you secret-share a function? Check out [Théo's Tutorial](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2011%20bis%20-%20Encrypted%20inference%20on%20ResNet-18.ipynb) for a nice intro on that!*\n",
    " \n",
    "\n",
    "### Base Implementation \n",
    "The concept of \"Function-Secret-Sharing\" (FSS) was first introduced in the [paper](https://eprint.iacr.org/2018/707): *Function Secret Sharing - Improvements and Extensions, by Elette Boyle and Niv Gilboa and Yuval Ishai, 2018* (This is the updated version)\n",
    "\n",
    "### AriaNN Implementation \n",
    "The PySyft Implementation is based on the [paper](https://arxiv.org/abs/2006.04593): *ARIANN: Low-Interaction Privacy-Preserving Deep Learning via Function Secret Sharing, by Théo Ryffel, David Pointcheval, Francis Bach, 2020* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Detail\n",
    "\n",
    "### Possible functions - High Level (Non exhaustive list)\n",
    "Resulting from the possible low-level computations (see \"curious section\" for more) the following standard models, optimizers and loss-functions can be used. *This should provide a useful summary, but doesn't claim to be an exhaustive list (feel free to add important items that can be composed out of the above mentioned possible low-level computations)\n",
    "\n",
    "* **Model Architecture:** Includes the possibility for encrypted computation of the derivative (needed for backprop)\n",
    "    * **Linear Layers** with SPDZ using beaver triples \n",
    "      * Matrix Multiplication and Convolutions (in CNNs e.g.)\n",
    "      * Average Pooling \n",
    "      * Batch-Norm/Normalization (approx. with Newton Method) \n",
    "      * Dropout \n",
    "    * **Non-Linear Layers:** mainly based on direct comparison capabilities\n",
    "      * Max Pooling \n",
    "      * ReLU, Leaky ReLU, Piece-wise linear activation functions \n",
    "      * Argmax \n",
    "        * **Beware** of using `.argmax()` on too many classes, 250 classes (~128 sec. to compute) was the maximum before my computer crashed because of too much memory consumption when using FSS encryption! (the encrypted comparison operation isn't memory efficient enough for now for datasets like ImageNet - 1000 classes - for now.)\n",
    "     \n",
    "* **Optimizers:**\n",
    "  * SGD (with Momentum)\n",
    "  * ADAM\n",
    "    * Not implemented yet (Oct. 2020)\n",
    "\n",
    "* **Loss-functions:** *No native torch losses are compatible, they have to be **manually** implemented!*  \n",
    "  * L1-Loss \n",
    "  * MSE \n",
    "  * Hinge Loss\n",
    "  * Cross-Entropy-Loss, Logistic Loss\n",
    "\n",
    "### Security Guarantees \n",
    "* Full Security includes **privacy and correctness**\n",
    "* The following guarantees hold for all settings where there is a majority of honest participating servers (**Not in dishonest majority setting!**)\n",
    "1. **Full Security for semi-honest corruption of a server** \n",
    "  * Privacy and Correctness of the data is secured if a server is being *honest-but-curious.* Meaning that follows the protocol but tries to infer as much information about the data it sees as possible. \n",
    "2. Potential Add-on: **Security with Abort**\n",
    "  * Protection against malicious servers can be guaranteed by adding [MAC authentication](https://en.wikipedia.org/wiki/Message_authentication_code) to the protocol. This would allow aborting the protocol as soon as one of the servers doesn't follow the protocol anymore. \n",
    "\n",
    "### Performance Evaluation\n",
    "* FSS doesn't require lots of communication rounds. But is computationally more intensive than SecureNN (because it includes calling a Pseudorandom Generator (PRG) many times) \n",
    "* Important Metrics: \n",
    "  * Round Complexity - *How many steps does the protocol involve. A step described as exchanging one message.* \n",
    "  * Communication Complexity - *How much bits are being exchanged during the protocol* \n",
    "* **DETAILS:** see curious section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_FSS(SimpleCNN_Base):\n",
    "    def __init__(self): \n",
    "        super(SimpleCNN_FSS, self).__init__()\n",
    "        \n",
    "        # Conv-Layers, Lin-Layers, Dropout from base class \n",
    "        # as the functionality stays the same for FSS, SNN \n",
    "        # both based on SPDZ \n",
    " \n",
    "        # feel free to comment these out if you want to use AvgPool from the parent class\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "        # Forward function doesn't change \n",
    "        \n",
    "simple_model_fss = SimpleCNN_FSS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration for encryption: 14.699193954467773\n"
     ]
    }
   ],
   "source": [
    "# create encrypted data_loader and encrypted model \n",
    "batch_size = 32\n",
    "# to make it quick we want to train only for 2 batches\n",
    "nr_batches = 2\n",
    "nr_samples = batch_size * nr_batches\n",
    "\n",
    "encryption_kwargs = dict(\n",
    "    workers=workers, \n",
    "    crypto_provider=crypto_provider, \n",
    "    protocol=\"fss\", \n",
    "    requires_grad=True,\n",
    "    precision_fractional= 4\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Note: Here we one-hot-encode the targets for multi-class one-vs-all classification training \n",
    "encrypted_train_loader = [\n",
    "        (data.encrypt(**encryption_kwargs), one_hot_of(target).encrypt(**encryption_kwargs))\n",
    "        for i, (data, target) in enumerate(cifar10_train_loader)\n",
    "        if i < nr_batches\n",
    "    ]\n",
    "\n",
    "# Note: Here we don't one-hot-encode targets, because we use targets only to calculate accuracy \n",
    "encrypted_test_loader = [\n",
    "        (data.encrypt(**encryption_kwargs), target.encrypt(**encryption_kwargs))\n",
    "        for i, (data, target) in enumerate(cifar10_test_loader)\n",
    "        if i < nr_batches\n",
    "    ]\n",
    "\n",
    "encrypted_model = simple_model_fss.encrypt(**encryption_kwargs)\n",
    "\n",
    "print(f\"Duration for encryption: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Nr. 0\n",
      "Batch Nr. 1\n",
      "Average Sample Loss: 718, Epoch Time: 76.00571823120117\n",
      "Batch Nr. 0\n",
      "Batch Nr. 1\n",
      "Average Sample Loss: 717, Epoch Time: 76.34843015670776\n"
     ]
    }
   ],
   "source": [
    "## Training ##\n",
    "\n",
    "nr_epochs = 2\n",
    "\n",
    "# Don't forget to also convert optim params into integers (no rational numb.) so that weights also stay integers\n",
    "optimizer = optim.SGD(encrypted_model.parameters(), lr=0.01, momentum=0.9).fix_precision(precision_fractional=4) \n",
    "\n",
    "epoch_loss = 0\n",
    "epoch_time = 0\n",
    "encrypted_model.train()\n",
    "\n",
    "for epoch in enumerate(range(nr_epochs)): \n",
    "    # Start timer \n",
    "    epoch_time = time.time()\n",
    "    for i, (input, target) in enumerate(encrypted_train_loader): \n",
    "        print(f\"Batch Nr. {i}\")\n",
    "        optimizer.zero_grad()\n",
    "        encrypted_result = encrypted_model(input)\n",
    "        # Manual MSELoss\n",
    "        #batch_loss = ((encrypted_result-target)**2).sum()\n",
    "        # Manual CELoss\n",
    "        batch_loss = CrossEntropyLoss(encrypted_result, target)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.get()\n",
    "        \n",
    "    print(f\"Average Sample Loss: {epoch_loss/nr_samples}, Epoch Time: {time.time() - epoch_time}\")\n",
    "    epoch_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Nr. 0\n",
      "Batch Nr. 1\n",
      "*************\n",
      "Test Accuracy: [0.109375],      Avg. Time Per Sample: 1.168 sec.\n"
     ]
    }
   ],
   "source": [
    "## Testing ##\n",
    "\n",
    "nr_correct = 0\n",
    "\n",
    "# Start timer \n",
    "start_time = time.time()\n",
    "encrypted_model.eval()\n",
    "\n",
    "for i, (input, target) in enumerate(encrypted_test_loader): \n",
    "    \n",
    "    print(f\"Batch Nr. {i}\")\n",
    "    \n",
    "    encrypted_result = encrypted_model(input)\n",
    "    \n",
    "    # calculate number of correct predictions (for accuracy) \n",
    "    encrypted_preds = encrypted_result.argmax(dim=1)\n",
    "    nr_correct += (encrypted_preds==target).sum()\n",
    "\n",
    "batch_times = time.time() - start_time\n",
    "\n",
    "print(\"*************\")\n",
    "print(f\"Test Accuracy: {nr_correct.decrypt().numpy()/nr_samples},\\\n",
    "      Avg. Time Per Sample: {batch_times/nr_samples :.4} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       "\t-> [PointerTensor | me:56204438389 -> sam:47442428026]\n",
       "\t-> [PointerTensor | me:60568323051 -> kelly:24972194743]\n",
       "\t*crypto provider: crypto_provider*"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Notable Results - Comparison \n",
    "Some example differences between the two protocols. Of course these results highly depend on how the different parameters are tuned and there are only very few training steps being considered. *This is a good place for you to start experiment for yourself!* \n",
    "<br>\n",
    "* Experiments: 64 samples with batch_size of 32, 2 training epochs \n",
    "  * **FSS:** \n",
    "    * **SGD + Momentum:** lr=0.01, momentum=0.9, **MSE-Loss**\n",
    "      * Avg.-Pool: Test Accuracy: 0.09380, Avg. time per sample: 0.833 sec. \n",
    "      * Max.-Pool: Test Accuracy: 0.09375, Avg. Time Per Sample: 1.433 sec.\n",
    "    * **SGD + Momentum:** lr=0.01, momentum=0.9, **Cross-Entropy-Loss**\n",
    "      * Avg.-Pool: Test Accuracy: **0.140625**, Avg. Time Per Sample: **0.733 sec.**\n",
    "      * Max.-Pool: Test Accuracy: 0.140625, Avg. Time Per Sample: 1.427 sec.\n",
    "    \n",
    "  * **SNN:** \n",
    "    * **SGD + Momentum:** lr=0.01, momentum=0.9, **MSE-Loss**\n",
    "      * Avg.-Pool: Test Accuracy: 0.09375, Avg. Time Per Sample: 4.624 sec.\n",
    "      * Max.-Pool: Test Accuracy: 0.09375, Avg. Time Per Sample: 7.83 sec.\n",
    "    * **SGD + Momentum:** lr=0.01, momentum=0.9, **Cross-Entropy-Loss**\n",
    "      * Avg.-Pool: Test Accuracy: **0.140625**, Avg. Time Per Sample: **4.242 sec.**\n",
    "      * Max.-Pool: Test Accuracy: 0.140625, Avg. Time Per Sample: 7.02 sec.\n",
    "\n",
    "*Note: to reproduce this experiments you might need to restart the run-time between the training of different models or using different encrpytions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Curious Section\n",
    "This section contains some extra details for those who want to understand the implementations of the protocols or who want to get deeper into the theoretical basics and respective papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SecureNN\n",
    "### Possible computations - Low Level\n",
    "This isn't a general-purpose protocol, which can compute all possible kinds of computations that are used in training NNs, but by giving up this flexibility (e.g. no usage of garbled circuits) we gain efficiency which is a vital criterion for the real-world-applicability of an encryption protocol.\n",
    "\n",
    "* Matrix Multiplication (SPDZ - Necessary Beaver Triples can also be generated for matrix multiplication)\n",
    "* Select Share\n",
    "  * Select one variable out of multiple private variables to be freshly masked and shared for a new computation. Used for e.g. for maxpool (i.e. select max element out of kernel-elements)\n",
    "* Private Compare\n",
    "  * Compare public variable with private variable. Used to compute the ReLU function (ReLU = max(x,0)) Used for e.g. for computation of MSB (see below)\n",
    "* Share Convert\n",
    "  * Convert private variables from one number space (a \"ring\") to another. Remember during computation the numbers are plain integers encoded by a bit-sequence of length L. After \"share convert\" they are encoded as a bit-sequence of length L-1. Used for e.g. for computation of derivative of ReLU.\n",
    "* Compute MSB (Most-Significant-Bit)\n",
    "  * Efficient reading of the sign-bit (is input integer positive or negative) mainly to compute the derivative of the ReLU function.\n",
    "* Non-linear functions:\n",
    "  * torch.log(), torch.exp()\n",
    "  \n",
    "### Security Guarantees \n",
    "* Full Security includes **privacy and correctness**\n",
    "* The following guarantees hold for all settings where there is a majority of honest participating servers (**Not in dishonest majority setting!**)\n",
    "    1. **Full Security for semi-honest corruption of a server** \n",
    "        * Privacy and Correctness of the data is secured if a server is being *honest-but-curious.* Meaning that follows the protocol but tries to infer as much information about the data it sees as possible. \n",
    "    2. **Privacy against malicious server** \n",
    "        * Even a server which doesn't follow the given protocol can't learn anything about the inputs and outputs of the other (honest) servers, *given that the majority of the participants are honest!* This is a common assumption for the malicious case because in a real setting deviating from the given protocol could be prevented with additional measures, as seen below.\n",
    "    3. Potential Add-on: **Security with Abort**\n",
    "        * Protection against malicious servers can be guaranteed by adding [MAC authentication](https://en.wikipedia.org/wiki/Message_authentication_code) to the protocol. This would allow aborting the protocol as soon as one of the servers doesn't follow the protocol anymore.\n",
    "\n",
    "### Performance Evaluation - Important Metrics \n",
    "* Division is possible but very slow! \n",
    "* Important Metrics: \n",
    "    * Round Complexity - *How many steps does the protocol involve. A step described as exchanging one message.* \n",
    "    * Communication Complexity - *How much bits are being exchanged during the protocol*\n",
    "\n",
    "See pictures from SecureNN paper.\n",
    "> \"The function\n",
    "Linear m,n,v denotes a matrix multiplication of dimen-\n",
    "sion m × n with n × v. Conv2dm, i,f,o denotes a convo-\n",
    "lutional layer with input m × m, i input channels, a\n",
    "filter of size f × f, and o output channels. lD denotes,\n",
    "precision of bits. Maxpooln and DMPn denotes Maxpool\n",
    "and its derivative over n elements. All communication is measured for l−bit inputs and p ,\n",
    "denotes the field size (which is 67 in our case)\"\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../material/SNN_complexity.png\" alt=\"SNN complexities\" width=\"90%\" height=\"auto\" /></td>\n",
    "        <td><img src=\"../material/SNN_dependencies.png\" alt=\"SNN dependencies\" width=\"90%\" height=\"auto\" /></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSS \n",
    "### Possible computations - Low Level \n",
    "Similar to SecureNN the current FSS protocol implemented in PySyft is focused on efficiency specifically for common computations in Machine Learning, thus leading to decreased flexibility but also allowing decreased computation time for relevant computations. <br>\n",
    "The following computations are supported: \n",
    "\n",
    "* Matrix Multiplication (SPDZ)\n",
    "* Equality Test \n",
    "  * It is checked whether a public value equals a private (i.e. shared) value \n",
    "* Comparison \n",
    "  * Inequality between a public value/expression and private value/expression\n",
    "* Non-linear functions\n",
    "  * torch.log(), torch.exp()\n",
    "\n",
    "\n",
    "### Performance Evaluation - Important Metrics \n",
    "* FSS doesn't require lots of communication rounds. But is computationally more intensive than SecureNN (because it includes calling a Pseudorandom Generator (PRG) many times) \n",
    "* Important Metrics: \n",
    "  * Round Complexity - *How many steps does the protocol involve. A step described as exchanging one message.* \n",
    "  * Communication Complexity - *How much bits are being exchanged during the protocol* \n",
    "  \n",
    "See the table from the above mentioned AriaNN paper as a good overview.\n",
    "  \n",
    "<img src=\"../material/FSS_complexity_comparison_to_FALCON.png\" width=\"60%\" height=\"auto\" alt=\"FSS complexity\" />\n",
    "\n",
    "[[48]](https://arxiv.org/abs/2004.02229) *Sameer Wagh, Shruti Tople, Fabrice Benhamouda, Eyal Kushilevitz, Prateek Mittal, and Tal Rabin. **Falcon:** Honest-majority maliciously secure framework for private deep learning. arXiv preprint arXiv:2004.02229, 2020.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aissue+is%3Aopen+label%3A%22Good+first+issue+%3Amortar_board%3A%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pp-ml] *",
   "language": "python",
   "name": "conda-env-pp-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
